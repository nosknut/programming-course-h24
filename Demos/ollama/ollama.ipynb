{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "Ollama is a simple way to download and run many open source LLM's locally. The smaller the LLM the faster and easier to run, however smaller models are also very stupid and often only understand specific tasks.\n",
    "\n",
    "- [Download](https://ollama.com/) the software and run one of the Ã²llama run` commands below to start using it.\n",
    "- [Docs](https://github.com/ollama/ollama?tab=readme-ov-file#ollama)\n",
    "- [API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md#api)\n",
    "- [Models](https://ollama.com/library)\n",
    "- Some more [models](https://github.com/ollama/ollama?tab=readme-ov-file#model-library) with size info\n",
    "- Suggestions for models that could possibly be run on a laptop:\n",
    "  - [llama3.1:8b](https://ollama.com/library/llama3.1:8b) from facebook 8 Billion parameter version (4.7Gb download)\n",
    "    - Run with command: `ollama run llama3.1:8b`\n",
    "  - [phi3:3.8b](https://ollama.com/library/phi3:3.8b) from Microsoft 3.8 Billion parameter version (2.2Gb download)\n",
    "    - Run with command: `ollama run phi3:3.8b` \n",
    "  - [phi3:14b](https://ollama.com/library/phi3:14b) from Microsoft 14 Billion parameter version (7.9Gb download)\n",
    "    - Run with command `ollama run phi3:14b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- [Download](https://ollama.com/) Ollama and run to start the server:\n",
    "    ```\n",
    "    ollama run phi3:3.8b\n",
    "    ```\n",
    "    - You can now chat with it directly in the terminal.\n",
    "    - To leave, type `/bye`.\n",
    "    If you wish to only serve the REST API, use the following command:\n",
    "    ```\n",
    "    ollama serve phi3:3.8b\n",
    "    ```\n",
    "    - To stop the server, close the ollama application from the task tray or task manager.\n",
    "    - For more commands, type `ollama --help` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": \"phi3:3.8b\",\n",
      "    \"created_at\": \"2024-08-19T12:46:48.3985307Z\",\n",
      "    \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm functioning within optimal parameters; thank you for asking. How about yourself?\"\n",
      "    },\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"done\": true,\n",
      "    \"total_duration\": 379279000,\n",
      "    \"load_duration\": 11799400,\n",
      "    \"prompt_eval_count\": 46,\n",
      "    \"prompt_eval_duration\": 42199000,\n",
      "    \"eval_count\": 19,\n",
      "    \"eval_duration\": 323309000\n",
      "}\n",
      "--------------------\n",
      "You asked: \n",
      "Hello, how are you?\n",
      "The model responded:\n",
      "I'm functioning within optimal parameters; thank you for asking. How about yourself?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def prompt_ollama(model, prompt, debug=False):\n",
    "    # Define the Ollama server URL\n",
    "    ollama_url = \"http://localhost:11434\"\n",
    "    \n",
    "    # Send a POST request to the Ollama server\n",
    "    # Url path to the HTTP POST endpoint of the Ollama REST API for generating chat responses\n",
    "    url = f\"{ollama_url}/api/chat\"\n",
    "    \n",
    "    # The JSON formatted data sent to the endpoint with the POST request, also known as the request body or payload\n",
    "    # https://github.com/ollama/ollama/blob/main/docs/api.md#examples-1\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": \"Be very concise. Do not add formatting or newlines to responses. Answer in at most 1-3 sentences.\" },\n",
    "            { \"role\": \"user\", \"content\": prompt },\n",
    "        ]        \n",
    "    }\n",
    "\n",
    "    # Call the REST API\n",
    "    response = requests.post(url, json=body)\n",
    "    \n",
    "    # Parse the body of the REST API response from text to JSON\n",
    "    response_body = response.json()\n",
    "    \n",
    "    if debug:\n",
    "        print(json.dumps(response_body, indent=4))\n",
    "    \n",
    "    # Get the generated output from the response\n",
    "    output = response_body[\"message\"][\"content\"]\n",
    "    \n",
    "    # Return the generated output\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "model_name = \"phi3:3.8b\"\n",
    "input_text = \"Hello, how are you?\"\n",
    "\n",
    "response = prompt_ollama(model_name, input_text, debug=True)\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"You asked: \")\n",
    "print(input_text)\n",
    "print(\"The model responded:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(prompt_ollama(model_name, \"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can this model be trusted?\n",
    "Run the cell below multiple times to find out.\n",
    "Larger models are more reliable and more consistent, but they will also lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest data available up until my last update in September 2021. No, I cannot access or predict real-time information beyond that point. Yes, I am designed with a knowledge cutoff mechanism preventing me from accessing new data post-September 2decade_two thousand twenty\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the most recent information you have?\n",
    "Give me only the month and the year\n",
    "Also yes/no, are you able to know anything after the point you were trained?\n",
    "\"\"\"\n",
    "\n",
    "response = prompt_ollama(model_name, prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
